{"cells":[{"cell_type":"markdown","source":["### Youtube comments analysis \n\nIn this notebook, we have a dataset of user comments for youtube videos related to animals or pets. We will attempt to identify cat or dog owners based on these comments, find out the topics important to them, and then identify video creators with the most viewers that are cat or dog owners."],"metadata":{}},{"cell_type":"markdown","source":["The dataset provided for this coding test are comments for videos related to animals and/or pets. The dataset is 240MB compressed and can be downloaded here:\nhttps://drive.google.com/file/d/1o3DsS3jN_t2Mw3TsV0i7ySRmh9kyYi1a/view?usp=sharing\n\n The dataset file is comma separated, with a header line defining the field names, listed here:\n \n‚óè creator_name. Name of the YouTube channel creator.\n‚óè userid. Integer identifier for the users commenting on the YouTube channels.\n‚óè comment. Text of the comments made by the users.\n\nStep 1: Identify Cat And Dog Owners\n\n‚óè Find the users who are cat and/or dog owners.\n\nStep 2: Build And Evaluate Classifiers\n\n‚óè Build classifiers for the cat and dog owners and measure the performance of the classifiers.\n\nStep 3: Classify All The Users\n\n‚óè Apply the cat/dog classifiers to all the users in the dataset. \n‚óè Estimate the fraction of all users who are cat/dog owners.\n\nStep 4: Extract Insights About Cat And Dog Owners\n\n‚óè Find topics popluar among cat and dog owners.\n\nStep 5: Identify Creators With Cat And Dog Owners In The Audience\n\n‚óè Find creators with the most cat and/or dog owners in the audience. Find creators with the highest statistically\nsignificant percentages of cat and/or dog owners."],"metadata":{}},{"cell_type":"markdown","source":["### 1. Import functions"],"metadata":{}},{"cell_type":"code","source":["%sh \npip install nltk\npip install --upgrade pip\npython -m nltk.downloader all"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting nltk\n  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\nRequirement already satisfied: six in /databricks/python3/lib/python3.7/site-packages (from nltk) (1.12.0)\nBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py): started\n  Building wheel for nltk (setup.py): finished with status &#39;done&#39;\n  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\nSuccessfully built nltk\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.4.5\nYou are using pip version 19.0.3, however version 19.3.1 is available.\nYou should consider upgrading via the &#39;pip install --upgrade pip&#39; command.\nCollecting pip\n  Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB)\nInstalling collected packages: pip\n  Found existing installation: pip 19.0.3\n    Uninstalling pip-19.0.3:\n      Successfully uninstalled pip-19.0.3\nSuccessfully installed pip-19.3.1\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading collection &#39;all&#39;\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/abc.zip.\n[nltk_data]    | Downloading package alpino to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/alpino.zip.\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n[nltk_data]    | Downloading package brown to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/brown.zip.\n[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/chat80.zip.\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/city_database.zip.\n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/cmudict.zip.\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/conll2000.zip.\n[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/conll2002.zip.\n[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/crubadan.zip.\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n[nltk_data]    | Downloading package dolch to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/dolch.zip.\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n[nltk_data]    | Downloading package floresta to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/floresta.zip.\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/genesis.zip.\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n[nltk_data]    | Downloading package ieer to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/ieer.zip.\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/inaugural.zip.\n[nltk_data]    | Downloading package indian to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/indian.zip.\n[nltk_data]    | Downloading package jeita to /root/nltk_data...\n[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/kimmo.zip.\n[nltk_data]    | Downloading package knbc to /root/nltk_data...\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n[nltk_data]    | Downloading package machado to /root/nltk_data...\n[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping models/moses_sample.zip.\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/names.zip.\n[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n[nltk_data]    | Downloading package omw to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/omw.zip.\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/paradigms.zip.\n[nltk_data]    | Downloading package pil to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/pil.zip.\n[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/pl196x.zip.\n[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/ppattach.zip.\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n[nltk_data]    | Downloading package propbank to /root/nltk_data...\n[nltk_data]    | Downloading package ptb to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/ptb.zip.\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n[nltk_data]    | Downloading package qc to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/qc.zip.\n[nltk_data]    | Downloading package reuters to /root/nltk_data...\n[nltk_data]    | Downloading package rte to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/rte.zip.\n[nltk_data]    | Downloading package semcor to /root/nltk_data...\n[nltk_data]    | Downloading package senseval to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/senseval.zip.\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n[nltk_data]    | Downloading package smultron to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/smultron.zip.\n[nltk_data]    | Downloading package state_union to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/state_union.zip.\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/stopwords.zip.\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/swadesh.zip.\n[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/switchboard.zip.\n[nltk_data]    | Downloading package timit to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/timit.zip.\n[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/toolbox.zip.\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/treebank.zip.\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n[nltk_data]    | Downloading package udhr to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/udhr.zip.\n[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/udhr2.zip.\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet.zip.\n[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n[nltk_data]    | Downloading package webtext to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/webtext.zip.\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet.zip.\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/words.zip.\n[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/ycoe.zip.\n[nltk_data]    | Downloading package rslp to /root/nltk_data...\n[nltk_data]    |   Unzipping stemmers/rslp.zip.\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n[nltk_data]    |   Unzipping help/tagsets.zip.\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping misc/perluniprops.zip.\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import when, col,explode\nfrom pyspark.ml.feature import RegexTokenizer, Word2Vec,StopWordsRemover,CountVectorizer\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator \nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom sklearn.metrics import confusion_matrix,roc_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### 2. Data Exploration and Cleaning"],"metadata":{}},{"cell_type":"markdown","source":["* Look at the data"],"metadata":{}},{"cell_type":"code","source":["df_clean=spark.read.csv(\"/FileStore/tables/animals_comments.csv\",inferSchema=True,header=True)\ndf_clean.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+-------------------------------------+\n        creator_name|userid|                              comment|\n+--------------------+------+-------------------------------------+\n        Doug The Pug|  87.0|                 I shared this to ...|\n        Doug The Pug|  87.0|                   Super cute  üòÄüêïüê∂|\n         bulletproof| 530.0|                 stop saying get e...|\n       Meu Zool√≥gico| 670.0|                 Tenho uma jiboia ...|\n              ojatro|1031.0|                 I wanna see what ...|\n     Tingle Triggers|1212.0|                 Well shit now Im ...|\nHope For Paws - O...|1806.0|                 when I saw the en...|\nHope For Paws - O...|2036.0|                 Holy crap. That i...|\n          Life Story|2637.0|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|\n       Brian Barczyk|2698.0|                 Call the teddy Larry|\n+--------------------+------+-------------------------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df_clean = df_clean.na.drop(subset=[\"comment\"])\ndf_clean_cnt = df_clean.count()\nprint('Total number of comments is :',df_clean_cnt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total number of comments is : 5818984\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["* Label the data"],"metadata":{}},{"cell_type":"code","source":["# find user with preference of dog and cat and label then with 1, otherwise 0.\ndf_clean = df_clean.withColumn(\"label\", \\\n                           (when(col(\"comment\").like(\"%my dog%\"), 1) \\\n                           .when(col(\"comment\").like(\"%I have a dog%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my cat%\"), 1) \\\n                           .when(col(\"comment\").like(\"%I have a cat%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my puppy%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my pup%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my kitty%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my pussy%\"), 1) \\\n                           .otherwise(0)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["df_clean.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+------+--------------------+-----+\n creator_name|userid|             comment|label|\n+-------------+------+--------------------+-----+\n Doug The Pug|  87.0|I shared this to ...|    0|\n Doug The Pug|  87.0|  Super cute  üòÄüêïüê∂|    0|\n  bulletproof| 530.0|stop saying get e...|    0|\nMeu Zool√≥gico| 670.0|Tenho uma jiboia ...|    0|\n       ojatro|1031.0|I wanna see what ...|    0|\n+-------------+------+--------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["label1_cnt = df_clean.filter(col('label')==1).count()\nlabel0_cnt = df_clean.filter(col('label')==0).count()\nprint('label 1 count:', label1_cnt)\nprint('label 0 count:', label0_cnt)\nprint('label 0 count/label 1 count:', label0_cnt // label1_cnt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">label 1 count: 40225\nlabel 0 count: 5778759\nlabel 0 count/label 1 count: 143\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### 2.1 Process data"],"metadata":{}},{"cell_type":"code","source":["# Establish pipeline and fit data\nregexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"words\", pattern=\"\\\\W\")\nword2Vec = Word2Vec(inputCol=\"words\", outputCol=\"features\")\npipeline = Pipeline(stages=[regexTokenizer, word2Vec])\n\npipelineFit = pipeline.fit(df_clean) # ~ 30 min\nalldata = pipelineFit.transform(df_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["alldata.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+------+--------------------+-----+--------------------+--------------------+\n creator_name|userid|             comment|label|               words|            features|\n+-------------+------+--------------------+-----+--------------------+--------------------+\n Doug The Pug|  87.0|I shared this to ...|    0|[i, shared, this,...|[0.00115683005953...|\n Doug The Pug|  87.0|  Super cute  üòÄüêïüê∂|    0|       [super, cute]|[-0.3668492734432...|\n  bulletproof| 530.0|stop saying get e...|    0|[stop, saying, ge...|[0.05945402098057...|\nMeu Zool√≥gico| 670.0|Tenho uma jiboia ...|    0|[tenho, uma, jibo...|[0.24394117854535...|\n       ojatro|1031.0|I wanna see what ...|    0|[i, wanna, see, w...|[0.02835624546489...|\n+-------------+------+--------------------+-----+--------------------+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Due to running time and memory limit, we will use small traning and test size."],"metadata":{}},{"cell_type":"code","source":["(lable1_train,lable1_test)=alldata.filter(col('label')==1).randomSplit([0.1, 0.9],seed = 42)[0].randomSplit([0.7, 0.3],seed = 100)\n(lable0_train, lable0_test)=alldata.filter(col('label')==0).randomSplit([0.001, 0.999],seed = 42)[0].randomSplit([0.7, 0.3],seed = 100)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["print('Number of label1_train, label0_train, label1_test, lable0_test:')\nprint(lable1_train.count(),lable0_train.count(), lable1_test.count(), lable0_test.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of label1_train, label0_train, label1_test, lable0_test:\n2841 4080 1224 1749\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["training = lable0_train.union(lable1_train)\ntest=lable0_test.union(lable1_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["### 3 Build the Classifier"],"metadata":{}},{"cell_type":"markdown","source":["### 3.1 Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["# Build estimator, LR model\nlr = LogisticRegression(maxIter=20)\n\n# Build parameter grid\nlr_paramGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n    .build()\nprint (\"Num models to be tested: \", len(lr_paramGrid))\n\n# Build the evaluator\nevaluator = BinaryClassificationEvaluator()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Num models to be tested:  6\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# Function1: Train model\ndef crossval_train(estimator,paramGrid,evaluator,training):\n  crossval = CrossValidator(estimator=estimator,\n                            estimatorParamMaps=paramGrid,\n                            evaluator=evaluator,\n                            numFolds=2)\n  cv_model = crossval.fit(training)\n  return cv_model\n\n\n# Function2: Predict\ndef predict_from_model(model,test):\n  best_model = model.bestModel\n  prediction = best_model.transform(test)\n  return best_model,prediction\n\n\n# Function3 Evaluate model performance\ndef evaluate_model(prediction):\n  #BinaryClassificationEvaluator has two metrics: areaUnderROC and areaUnderPR\n  evaluator =  BinaryClassificationEvaluator()\n  areaunderAOC = evaluator.evaluate(prediction)\n  areaunderPR = evaluator.setMetricName(\"areaUnderPR\").evaluate(prediction)\n  \n  #MulticlassClassificationEvaluator \n  evaluator_multi = MulticlassClassificationEvaluator()\n  f1 = evaluator_multi.setMetricName(\"f1\").evaluate(prediction)\n  weightedPrecision = evaluator_multi.setMetricName(\"weightedPrecision\").evaluate(prediction)\n  weightedRecall = evaluator_multi.setMetricName(\"weightedRecall\").evaluate(prediction)\n  accuracy = evaluator_multi.setMetricName(\"accuracy\").evaluate(prediction)\n  \n  return [f1,weightedPrecision,weightedRecall,accuracy,areaunderAOC,areaunderPR]\n\n\n# Function4 Print model performance\ndef print_model_performance(result):\n  print ('**Best LR Model Evalution**')\n  print(' F1:',result[0])\n  print(' Precision:',result[1])\n  print(' Recall:',result[2])\n  print(' Accuracy:',result[3])\n  print(' AreaUnderAOC:',result[4])\n  print(' AreaUnderPR:',result[5])\n  \n# Function5 Select partial data\ndef select(prediction):\n  selected_pd = prediction.select('userid', 'comment', 'label','probability','prediction').toPandas()\n  selected_pd['probability_of_pos'] = selected_pd['probability'].apply(lambda x: x[1])\n  selected_pd['prediction'] = selected_pd['prediction'].apply(lambda x: int(x))\n  return selected_pd \n\n# Function6 Generate confusion matrix and ROC\ndef metrics(selected_pd):\n  cm = confusion_matrix(selected_pd['label'], selected_pd['prediction'])/selected_pd.count()[0] # normalization\n  fpr,tpr,thres = roc_curve(selected_pd['label'],selected_pd['probability_of_pos'],pos_label=1)\n  return cm, fpr,tpr\n\n# Function7 Plot confusion matrix\ndef plot_cm(cm,model_name):\n  cm_pd_df = pd.DataFrame(cm, range(2), range(2))\n  fig,ax = plt.subplots(figsize = (5,4))\n  sns.heatmap(cm_pd_df,cmap = 'Spectral_r',annot = True , square = True)\n  plt.xlabel('Predicted')\n  plt.ylabel('Actual')\n  plt.title('Confusion Matrix of {}'.format(model_name))\n  display()\n  \n# Function8 Plot ROC curve\ndef plot_roc(fpr,tpr,model_name):  \n  fig,ax = plt.subplots(figsize = (5,4))\n  sns.lineplot([0, 1], [0, 1],color = 'black',ax = ax)\n  sns.lineplot(fpr, tpr, color = 'purple',ax = ax)\n  plt.xlabel('False positive rate')\n  plt.ylabel('True positive rate')\n  plt.title('ROC curve of {}'.format(model_name))\n  plt.legend(loc='best')\n  display()\n\n# Function9 Plot cm and ROC curve\ndef plot_cm_roc(cm,fpr,tpr,model_name):\n  cm_pd_df = pd.DataFrame(cm, range(2), range(2))\n  fig,ax = plt.subplots(1,2,figsize = (10,4))\n  sns.heatmap(cm_pd_df,cmap = 'Spectral_r',annot = True , square = True,ax = ax[0])\n  sns.lineplot([0, 1], [0, 1],color = 'black',ax = ax[1])\n  sns.lineplot(fpr, tpr, color = 'purple',ax = ax[1])\n  ax[0].set_xlabel('Predicted')\n  ax[0].set_ylabel('Actual')\n  ax[1].set_xlabel('False positive rate')\n  ax[1].set_ylabel('True positive rate')\n  ax[0].set_title('Confusion Matrix and ROC of {}'.format(model_name))\n  ax[1].set_title('ROC curve of {}'.format(model_name))\n  display()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["lr_model = crossval_train(lr,lr_paramGrid,evaluator,training) # runs ~27 min\nbest_lr_model,lr_prediction = predict_from_model(lr_model,test)\nlr_result= evaluate_model(lr_prediction)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["#Print best model parameters and model performance\nprint ('**Best LR Model**')\nprint (' RegParam:',best_lr_model._java_obj.parent().getRegParam()) #parent()method will return an estimator, get the best params \nprint (' ElasticNetParam:',best_lr_model._java_obj.parent().getElasticNetParam())\n\nprint_model_performance(lr_result)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">**Best LR Model**\n RegParam: 0.01\n ElasticNetParam: 0.0\n**Best LR Model Evalution**\n F1: 0.8885451201980149\n Precision: 0.8889983215896022\n Recall: 0.8883282879246552\n Accuracy: 0.8883282879246552\n AreaUnderAOC: 0.9523074810255716\n AreaUnderPR: 0.9134965605700793\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["# plot confusion matrix and ROC curve\nlr_selected_pd = select(lr_prediction)\nlr_cm, lr_fpr,lr_tpr = metrics(lr_selected_pd)\nplot_cm_roc(lr_cm,lr_fpr,lr_tpr,'LR')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### 3.2 RandomForest"],"metadata":{}},{"cell_type":"code","source":["rf = RandomForestClassifier()\nrf_paramGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [5,10])\n             .addGrid(rf.maxBins, [20])\n             .addGrid(rf.numTrees, [5,10,20])\n             .build())\nprint (\"Num models to be tested: \", len(rf_paramGrid))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Num models to be tested:  6\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["rf_model = crossval_train(rf,rf_paramGrid,evaluator,training) # runs ~ 32 min\nbest_rf_model,rf_prediction = predict_from_model(rf_model,test)\nrf_result= evaluate_model(rf_prediction)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["#Print model parameters\nprint ('**Best RF Model**')\nprint (' MaxDepth:',best_rf_model._java_obj.parent().getMaxDepth())\nprint (' MaxBins:',best_rf_model._java_obj.parent().getMaxBins()) #parent()method will return an estimator, get the best params \nprint (' NumTrees:',best_rf_model._java_obj.parent().getNumTrees())\n\nprint_model_performance(rf_result)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">**Best RF Model**\n MaxDepth: 10\n MaxBins: 20\n NumTrees: 20\n**Best LR Model Evalution**\n F1: 0.9064891252206594\n Precision: 0.907755115050942\n Recall: 0.9061553985872857\n Accuracy: 0.9061553985872856\n AreaUnderAOC: 0.9590839957099665\n AreaUnderPR: 0.9313391924057399\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["# plot confusion matrix and ROC curve\nrf_selected_pd = select(rf_prediction)\nrf_cm, rf_fpr,rf_tpr = metrics(rf_selected_pd)\nplot_cm_roc(rf_cm,rf_fpr,rf_tpr,'RF')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["best_rf_model.featureImportances()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### 3.3 Gradient boosting trees"],"metadata":{}},{"cell_type":"code","source":["## Unable to implement paramGrid for GBT due to OutofMemory Error\n'''\ngbt = GBTClassifier(maxIter=3)\ngbt_paramGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 5])\n             .addGrid(gbt.maxBins, [10, 20])\n             .build())\n\ngbt_model = crossval_train(gbt,gbt_paramGrid,evaluator,training)  # runs 51.44 min\nbest_gbt_model,gbt_prediction = predict_from_model(gbt_model,test)\ngbt_result= evaluate_model(gbt_prediction)\n'''\n\n## Print model parameters\n'''\nprint ('**Best GBT Model**')\nprint (' MaxDepth:',best_gbt_model._java_obj.parent().getMaxDepth())\nprint (' MaxBins:',best_gbt_model._java_obj.parent().getMaxBins()) \n'''\n\n## Print model performance\n'''\nprint_model_performance(gbt_result)\n'''"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["## Unable to implement paramGrid for GBT due to OutofMemory Error\ngbt = GBTClassifier(maxIter=20,maxDepth=10,maxBins=20) #36mins\ngbt_model = gbt.fit(training)\ngbt_prediction = gbt_model.transform(test)\ngbt_result= evaluate_model(gbt_prediction)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["print_model_performance(gbt_result)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">**Best LR Model Evalution**\n F1: 0.8410652369149991\n Precision: 0.8492356334647546\n Recall: 0.839892364614867\n Accuracy: 0.8398923646148672\n AreaUnderAOC: 0.9354051988624681\n AreaUnderPR: 0.9070928414893008\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["gbt_selected_pd = select(gbt_prediction)\ngbt_cm, gbt_fpr,gbt_tpr = metrics(gbt_selected_pd)\nplot_cm_roc(gbt_cm,gbt_fpr,gbt_tpr, 'GBT')"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### 3.4 Compare between models"],"metadata":{}},{"cell_type":"code","source":["def plot_cm_all(cm_list,model_name_list):\n  fig,ax = plt.subplots(1,len(cm_list),figsize = (12,4),sharey = True)\n  count = 0\n  cbar_ax = fig.add_axes([.03, .2, .01, .5])\n  while count < len(cm_list):\n    cm = cm_list[count]\n    cm_pd_df = pd.DataFrame(cm, range(2), range(2))\n    ax[count].set_title(model_name_list[count])\n    sns.heatmap(cm_pd_df,cmap = 'Spectral_r',annot = True , square = True, ax = ax[count],cbar = count==0,cbar_ax = cbar_ax if count == 0 else None)\n    if count == 0:\n      ax[count].set_xlabel('Predicted')\n      ax[count].set_ylabel('Actual')\n    count+=1\n  display()\n  \n  \ndef plot_roc_all(fpr_list,tpr_list,model_name_list): \n  color_list = ['blue','red','purple']\n  fig,ax = plt.subplots(figsize = (5,4))\n  sns.lineplot([0, 1], [0, 1],color = 'black',ax = ax)\n  count = 0\n  while count < len(fpr_list):\n    fpr = fpr_list[count]\n    tpr = tpr_list[count]\n    model_name = model_name_list[count]\n    color = color_list[count]\n    sns.lineplot(fpr, tpr, color = color,label = model_name,ax = ax)\n    count += 1\n  plt.xlabel('False positive rate')\n  plt.ylabel('True positive rate')\n  plt.title('ROC curve')\n  plt.legend(loc='best')\n  display()\n  \ndef print_model_performance_all(LR_result,RF_result,GBT_result):\n  index = ['F1','Precision','Recall','Accuracy','AreaUnderROC','AreaUnderPR']\n  result_df = pd.DataFrame(list(zip(LR_result,RF_result,GBT_result)))\n  result_df.columns = ['LR','RF','GBT']\n  result_df.index = index\n  return result_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"code","source":["print_model_performance_all(lr_result,rf_result,gbt_result).head(6)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LR</th>\n      <th>RF</th>\n      <th>GBT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>F1</th>\n      <td>0.888545</td>\n      <td>0.906489</td>\n      <td>0.841065</td>\n    </tr>\n    <tr>\n      <th>Precision</th>\n      <td>0.888998</td>\n      <td>0.907755</td>\n      <td>0.849236</td>\n    </tr>\n    <tr>\n      <th>Recall</th>\n      <td>0.888328</td>\n      <td>0.906155</td>\n      <td>0.839892</td>\n    </tr>\n    <tr>\n      <th>Accuracy</th>\n      <td>0.888328</td>\n      <td>0.906155</td>\n      <td>0.839892</td>\n    </tr>\n    <tr>\n      <th>AreaUnderROC</th>\n      <td>0.952307</td>\n      <td>0.959084</td>\n      <td>0.935405</td>\n    </tr>\n    <tr>\n      <th>AreaUnderPR</th>\n      <td>0.913497</td>\n      <td>0.931339</td>\n      <td>0.907093</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["cm_list = [lr_cm,rf_cm,gbt_cm]\nfpr_list = [lr_fpr,rf_fpr, gbt_fpr]\ntpr_list = [lr_tpr,rf_tpr, gbt_tpr]\nmodel_name_list = ['LR','RF','GBT']\nplot_cm_all(cm_list,model_name_list)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["plot_roc_all(fpr_list,tpr_list,model_name_list)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["We will choose random forest model to classify all the users as we see it has the best performance."],"metadata":{}},{"cell_type":"markdown","source":["#### 4. Classify All The Users"],"metadata":{}},{"cell_type":"code","source":["rf_prediction_all = best_rf_model.transform(alldata)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":46},{"cell_type":"code","source":["cnt1 = rf_prediction_all.filter(col('prediction')==1).count()\ncnt0 = rf_prediction_all.filter(col('prediction')==0).count()\nprint('Overall, there are {} cat/dog owners and {} non cat/dog owners.'.format(cnt1,cnt0))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Overall, there are 633651 cat/dog owners and 5185333 non cat/dog owners.\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["fig,ax = plt.subplots(figsize = (5,4))\nax.pie([cnt1,cnt0],labels = ['owners', 'non-owners'],startangle=90,counterclock=False,autopct='%1.0f%%')\nax.set_title('Cat/dogs owner percentage')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### 5. Find most popular topics among cat/dog owners -LDA"],"metadata":{}},{"cell_type":"code","source":["stopword_list = stopwords.words('english')\nstopword_list.extend(['dogs','cats'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["regexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"words\", pattern=\"\\\\W\")\nremover = StopWordsRemover(inputCol='words', outputCol='words_clean',stopWords = stopword_list)\nvectorizer = CountVectorizer(inputCol='words_clean', outputCol=\"features\")\npipeline_lda = Pipeline(stages=[regexTokenizer,remover,vectorizer])\npipeline_lda_model = pipeline_lda.fit(df_clean)\ndf_lda= pipeline_lda_model.transform(df_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["# choose only cat/dog owners\ndf_lda_1 = df_lda.filter(col('label')==1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":52},{"cell_type":"code","source":["# choose Non cat/dog owners\ndf_lda_0 = df_lda.filter(col('label')==0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":53},{"cell_type":"code","source":["lda = LDA(k=5, maxIter=20) # 4 mins\nlda1_model = lda.fit(df_lda_1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":54},{"cell_type":"code","source":["lda0 = LDA(k=5, maxIter=5)\nlda0_model = lda0.fit(df_lda_0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":55},{"cell_type":"code","source":["topics_1 = lda1_model.describeTopics(maxTermsPerTopic = 6)\ntopics_0 = lda0_model.describeTopics(maxTermsPerTopic = 6)\ntopics_1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+--------------------+\ntopic|         termIndices|         termWeights|\n+-----+--------------------+--------------------+\n    0|[10, 1, 33, 6, 52...|[0.00229862012715...|\n    1|[10, 33, 8, 1, 2, 4]|[8.46915728906857...|\n    2|[33, 1, 52, 0, 10...|[0.00175953329901...|\n    3|[33, 1, 2, 0, 18,...|[4.19355380283055...|\n    4|[10, 33, 1, 0, 4, 5]|[0.02611769925833...|\n+-----+--------------------+--------------------+\n\n</div>"]}}],"execution_count":56},{"cell_type":"code","source":["topics_0.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+--------------------+\ntopic|         termIndices|         termWeights|\n+-----+--------------------+--------------------+\n    0| [1, 4, 3, 6, 11, 2]|[0.01572995786379...|\n    1|[9, 0, 62, 58, 8,...|[0.01220163145276...|\n    2| [0, 7, 2, 1, 14, 5]|[0.01955066178877...|\n    3|[19, 30, 2, 0, 1,...|[0.01724216368241...|\n    4|[0, 27, 13, 77, 2...|[0.01730877181601...|\n+-----+--------------------+--------------------+\n\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["vectorizer_model = pipeline_lda_model.stages[2]\nvocab_list = vectorizer_model.vocabulary"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":58},{"cell_type":"code","source":["id_to_str = udf(lambda x : [vocab_list[i] for i in x])\nround_term_weights = udf(lambda x : [round(i,5) for i in x])\n\ntopics_1 = topics_1 \\\n           .withColumn(\"terms\", id_to_str(\"termIndices\")) \\\n           .withColumn(\"weights\", round_term_weights(\"termWeights\")) \\\n           .select(\"topic\", \"terms\", \"weights\")\n\ntopics_0 = topics_0 \\\n           .withColumn(\"terms\", id_to_str(\"termIndices\")) \\\n           .withColumn(\"weights\", round_term_weights(\"termWeights\")) \\\n           .select(\"topic\", \"terms\", \"weights\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":59},{"cell_type":"markdown","source":["Popular topics among cat/dog owners"],"metadata":{}},{"cell_type":"code","source":["topics_1.toPandas().head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>terms</th>\n      <th>weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[dog, like, cat, would, looks, one]</td>\n      <td>[0.0023, 0.00198, 9.3E-4, 7.6E-4, 7.2E-4, 5.9E-4]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[dog, cat, dont, like, im, one]</td>\n      <td>[8.5E-4, 5.0E-4, 4.1E-4, 4.0E-4, 3.6E-4, 3.1E-4]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[cat, like, looks, love, dog, im]</td>\n      <td>[0.00176, 6.3E-4, 3.5E-4, 2.6E-4, 2.5E-4, 1.9E-4]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[cat, like, im, love, much, think]</td>\n      <td>[4.2E-4, 1.1E-4, 8.0E-5, 7.0E-5, 6.0E-5, 6.0E-5]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[dog, cat, like, love, one, get]</td>\n      <td>[0.02612, 0.01486, 0.00984, 0.0058, 0.00546, 0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":61},{"cell_type":"markdown","source":["Popular topics among Non cat/dog owners"],"metadata":{}},{"cell_type":"code","source":["topics_0.toPandas().head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>terms</th>\n      <th>weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[like, one, video, would, great, im]</td>\n      <td>[0.01573, 0.00853, 0.00763, 0.00734, 0.00631, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[cute, love, que, de, dont, el]</td>\n      <td>[0.0122, 0.00953, 0.00752, 0.00727, 0.00578, 0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[love, good, im, like, videos, get]</td>\n      <td>[0.01955, 0.01063, 0.00996, 0.00991, 0.00908, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[coyote, 3, im, love, like, wow]</td>\n      <td>[0.01724, 0.01347, 0.01017, 0.00982, 0.00659, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[love, n, u, c, lol, ng]</td>\n      <td>[0.01731, 0.01573, 0.01039, 0.00985, 0.00679, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":63},{"cell_type":"markdown","source":["#### 6. Identify Creators With the Most Cat And Dog Owners In The Audience"],"metadata":{}},{"cell_type":"code","source":["creators = rf_prediction_all \\\n           .filter(rf_prediction_all.prediction==1) \\\n           .groupBy('creator_name','userid') \\\n           .count() \\\n           .withColumnRenamed('count','number of comments') \\\n           .orderBy('number of comments',ascending = False) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":65},{"cell_type":"markdown","source":["Active users - Top 10 owners who comment most"],"metadata":{}},{"cell_type":"code","source":["creators.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+------------------+\n        creator_name|   userid|number of comments|\n+--------------------+---------+------------------+\n     Viktor Larkhill|1358915.0|               226|\n           BrookIvy3|  17165.0|               218|\n            The Dodo| 575285.0|               194|\n           BrookIvy3|2128217.0|               187|\n           BrookIvy3|  10199.0|               179|\n            The Dodo|1194456.0|               176|\n           BrookIvy3|1293328.0|               170|\n        Robin Seplut| 759311.0|               168|\n           BrookIvy3| 768839.0|               157|\nMax and Katie the...| 587110.0|               134|\n+--------------------+---------+------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":67},{"cell_type":"code","source":["owners_under_creators = creators \\\n                        .groupBy('creator_name') \\\n                        .count() \\\n                        .withColumnRenamed('count','number of owners') \\\n                        .orderBy('count',ascending = False)  # 174148 owners in total"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":68},{"cell_type":"markdown","source":["Top 10 creators with the most owners"],"metadata":{}},{"cell_type":"code","source":["owners_under_creators.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------------+\n        creator_name|number of owners|\n+--------------------+----------------+\n    Brave Wilderness|           42789|\n            The Dodo|           37918|\n  Taylor Nicole Dean|           29679|\nHope For Paws - O...|           20377|\n       Brian Barczyk|           19971|\n     Gohan The Husky|           16141|\n        Robin Seplut|           15463|\n           Vet Ranch|           15277|\n    Cole &amp; Marmalade|           10753|\n          stacyvlogs|           10116|\n+--------------------+----------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":70},{"cell_type":"markdown","source":["### Summary"],"metadata":{}},{"cell_type":"markdown","source":["In this project, we analyzed a dataset of user comments on youtube videos related to pets(cats and dogs). We hope to identify users with pets and topics interesting to them.\n\nTo achieve the goal, we first take a look at the data, remove the missing values and label the data. Specifically, we identify users commenting like 'my dog', 'I have a dog' , 'I have a cat' etc as pets owners. Of course, we might miss some onwers. By trainign a model, we hope to identify as many users. \n\nBefore training, we need to process the data to be ready. So we converted the comments texts to feature vectors using RegTokenizer and Word2Vec in Spark ML.Then, we trained the data with Logistic regression, random foerest and graident boosting tree models. Among all, we find that LR and RF give us good performance, evaluated based on accuracy, F1 score, AOC and etc. In addition, we extracted the confusion matrix and ROC cureve for visulization of model performance.  Considering the overall model performance, we select RF model to apply in the final dataset.\n\nIt is found that only about 11% of the total users are pets owners. So I concluded that most users don't have a cat or dog. Therefore, I believe it would be quite helpful to find topics that are interesting to these users in order to gain more views on the videos. This was inplemented using Latent Dirichlet Algolocation(LDA) learning model. And interesting topics are displayed in the results"],"metadata":{}}],"metadata":{"name":"Spark HW3 Youtube data analysis","notebookId":3101603999008160},"nbformat":4,"nbformat_minor":0}
